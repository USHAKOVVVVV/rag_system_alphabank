{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da49c25",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abaf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input_data/websites.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f489e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('input_data/questions_clean.csv')\n",
    "df_questions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376e30b",
   "metadata": {},
   "source": [
    "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc622d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e65370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò\n",
    "print(\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\")\n",
    "model = SentenceTransformer('ai-forever/sbert_large_nlu_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726aa645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset  # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç Dataset\n",
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏–∑ JSON —Ñ–∞–π–ª–∞\n",
    "def load_training_data(file_path='education_pairs.json'):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏–∑ JSON —Ñ–∞–π–ª–∞\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            training_data = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(training_data)} –ø–∞—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö\n",
    "        for i, item in enumerate(training_data):\n",
    "            if not all(key in item for key in ['text1', 'text2', 'score']):\n",
    "                print(f\"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –≤ —ç–ª–µ–º–µ–Ω—Ç–µ {i} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è\")\n",
    "                \n",
    "        return training_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "training_data = load_training_data('education_pairs.json')\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "class SimilarityDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.examples = []\n",
    "        \n",
    "        for item in data:\n",
    "            self.examples.append(InputExample(\n",
    "                texts=[item['text1'], item['text2']],\n",
    "                label=item['score']\n",
    "            ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3937526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def fine_tune_triplets_simple():\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∏–ø–ª–µ—Ç–∞—Ö —á–µ—Ä–µ–∑ encode\"\"\"\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã\n",
    "    with open('triplets.json', 'r', encoding='utf-8') as f:\n",
    "        triplets_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(triplets_data)} —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    try:\n",
    "        model = SentenceTransformer('fine-tuned-model')\n",
    "        print(\"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–Ω–µ–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\")\n",
    "    except:\n",
    "        model = SentenceTransformer('ai-forever/sbert_large_nlu_ru')\n",
    "        print(\"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä–ø—É—Å –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤\n",
    "    corpus = []\n",
    "    for triplet in triplets_data:\n",
    "        corpus.append(triplet['anchor'])\n",
    "        corpus.append(triplet['positive']) \n",
    "        corpus.append(triplet['negative'])\n",
    "    \n",
    "    print(\"üéØ –î–æ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç—Ä–∏–ø–ª–µ—Ç–∞—Ö...\")\n",
    "    \n",
    "    # –ü—Ä–æ—Å—Ç–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ encode\n",
    "    for epoch in range(3):\n",
    "        embeddings = model.encode(\n",
    "            corpus,\n",
    "            batch_size=5,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        print(f\"üìä –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "    \n",
    "    model.save('fine-tuned-with-triplets')\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ç—Ä–∏–ø–ª–µ—Ç–∞—Ö –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")\n",
    "\n",
    "fine_tune_triplets_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b80894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ JSON —Ñ–∞–π–ª–∞\n",
    "with open('output_chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b67874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞\n",
    "    \"\"\"\n",
    "    # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –±—É–∫–≤—ã\n",
    "    text = re.sub(r'[|\\[\\](){}¬´¬ª\"''`]', ' ', text)\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def prepare_chunk_text(chunk):\n",
    "    \"\"\"\n",
    "    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π\n",
    "    \"\"\"\n",
    "    title = chunk['title']\n",
    "    text = chunk['text']\n",
    "    \n",
    "    # –£—Å–ª–æ–≤–∏–µ: –µ—Å–ª–∏ title –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ .pdf - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ\n",
    "    if title.lower().endswith('.pdf'):\n",
    "        combined_text = text\n",
    "    else:\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º title –∏ text, —Ä–∞–∑–¥–µ–ª—è—è —Ç–æ—á–∫–æ–π\n",
    "        combined_text = f\"{title}. {text}\"\n",
    "    \n",
    "    # –û—á–∏—â–∞–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "    cleaned_text = clean_text(combined_text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8336c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "documents = []\n",
    "metadata = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞\n",
    "\n",
    "for chunk in chunks:\n",
    "    cleaned_text = prepare_chunk_text(chunk)\n",
    "    documents.append(cleaned_text)\n",
    "    \n",
    "    metadata.append({\n",
    "        'chunk_id': chunk['chunk_id'],\n",
    "        'source': chunk['source'],\n",
    "        'title': chunk['title'],\n",
    "        'word_count': len(cleaned_text.split())\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fde9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –ß–ê–ù–ö–û–í\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —á–∞–Ω–∫–∏...\")\n",
    "chunk_embeddings = model.encode(\n",
    "    documents,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b24af6",
   "metadata": {},
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ qdrant, –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö web —Å—Ç—Ä–∞–Ω–∏—Ü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "def wait_for_qdrant(max_retries=30):\n",
    "    \"\"\"–ñ–¥–µ—Ç –ø–æ–∫–∞ Qdrant –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è\"\"\"\n",
    "    print(\"–û–∂–∏–¥–∞–µ–º –∑–∞–ø—É—Å–∫ Qdrant...\")\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            client = QdrantClient(host=\"localhost\", port=6333)\n",
    "            client.get_collections()\n",
    "            print(\"Qdrant –≥–æ—Ç–æ–≤!\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            if i % 5 == 0:\n",
    "                print(f\"   –ü–æ–ø—ã—Ç–∫–∞ {i+1}/{max_retries}...\")\n",
    "            time.sleep(2)\n",
    "    raise Exception(\"Qdrant –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è\")\n",
    "\n",
    "def find_all_json_files(data_folder=\"data\"):\n",
    "    \"\"\"–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ JSON —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ data\"\"\"\n",
    "    json_pattern = os.path.join(data_folder, \"*.json\")\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    print(f\"üîç –ü–æ–∏—Å–∫ JSON —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {data_folder}:\")\n",
    "    for file in json_files:\n",
    "        print(f\"   üìÑ {file}\")\n",
    "    \n",
    "    return json_files\n",
    "\n",
    "def load_all_json_data_with_ids(data_folder=\"data\"):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ JSON —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ data —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ ID\"\"\"\n",
    "    json_files = find_all_json_files(data_folder)\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"‚ùå –í –ø–∞–ø–∫–µ {data_folder} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤\")\n",
    "        return None\n",
    "    \n",
    "    all_data = []\n",
    "    id_offset = 0\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"\\nüìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª: {json_file}\")\n",
    "        \n",
    "        if not os.path.exists(json_file):\n",
    "            print(f\"   ‚ùå –§–∞–π–ª {json_file} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "            for item in data:\n",
    "                item['unique_id'] = id_offset + item.get('chunk_id', 0)\n",
    "                item['file_source'] = os.path.basename(json_file)\n",
    "            \n",
    "            print(f\"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —á–∞–Ω–∫–æ–≤ (ID: {id_offset} - {id_offset + len(data) - 1})\")\n",
    "            all_data.extend(data)\n",
    "            id_offset += len(data)  # –°–º–µ—â–µ–Ω–∏–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤: {len(all_data)}\")\n",
    "    print(f\"üìä –î–∏–∞–ø–∞–∑–æ–Ω ID: 0 - {len(all_data) - 1}\")\n",
    "    return all_data\n",
    "\n",
    "def setup_json_collection(collection_name=\"json_chunks\"):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è JSON –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    try:\n",
    "        # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é\n",
    "        client.delete_collection(collection_name)\n",
    "        print(f\"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # –í—Ä–µ–º–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ–º —Å —Ä–∞–∑–º–µ—Ä–æ–º 384, –ø–æ—Ç–æ–º –ø–µ—Ä–µ—Å–æ–∑–¥–∞–¥–∏–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(f\"üÜï –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}\")\n",
    "    \n",
    "    return collection_name\n",
    "\n",
    "def check_collection_info(collection_name=\"json_chunks\"):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\"\"\"\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    try:\n",
    "        info = client.get_collection(collection_name)\n",
    "        print(f\"üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}':\")\n",
    "        print(f\"   ‚Ä¢ –¢–æ—á–µ–∫: {info.points_count}\")\n",
    "        print(f\"   ‚Ä¢ –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {info.config.params.vectors.size}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
    "        points = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=5\n",
    "        )[0]\n",
    "        \n",
    "        print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "        for i, point in enumerate(points):\n",
    "            print(f\"   –ü—Ä–∏–º–µ—Ä {i+1}:\")\n",
    "            print(f\"     ID: {point.id}\")\n",
    "            print(f\"     –ò—Å—Ç–æ—á–Ω–∏–∫ —Ñ–∞–π–ª–∞: {point.payload.get('file_source', 'unknown')}\")\n",
    "            print(f\"     URL: {point.payload.get('source', '')}\")\n",
    "            print(f\"     –ó–∞–≥–æ–ª–æ–≤–æ–∫: {point.payload.get('title', '')[:50]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}\")\n",
    "\n",
    "def verify_collection_count(collection_name=\"json_chunks\", expected_count=20732):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\"\"\"\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    try:\n",
    "        info = client.get_collection(collection_name)\n",
    "        actual_count = info.points_count\n",
    "        \n",
    "        print(f\"\\nüîç –ü–†–û–í–ï–†–ö–ê –ö–û–õ–ò–ß–ï–°–¢–í–ê:\")\n",
    "        print(f\"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_count} —Ç–æ—á–µ–∫\")\n",
    "        print(f\"   –§–∞–∫—Ç–∏—á–µ—Å–∫–∏: {actual_count} —Ç–æ—á–µ–∫\")\n",
    "        \n",
    "        if actual_count == expected_count:\n",
    "            print(f\"   ‚úÖ –í–°–ï –¢–û–ß–ö–ò –ó–ê–ì–†–£–ñ–ï–ù–´!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç {expected_count - actual_count} —Ç–æ—á–µ–∫\")\n",
    "            \n",
    "        return actual_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_and_upload_json_fixed(data, collection_name=\"json_chunks\", batch_size=100):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç JSON –¥–∞–Ω–Ω—ã–µ –≤ Qdrant —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ ID\"\"\"\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏\")\n",
    "        return 0\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –ø–æ –ø–µ—Ä–≤–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É\n",
    "    first_item = data[0]\n",
    "    if 'embedding' in first_item and first_item['embedding']:\n",
    "        vector_size = len(first_item['embedding'])\n",
    "        print(f\"üìè –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {vector_size}\")\n",
    "        \n",
    "        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –≤–µ–∫—Ç–æ—Ä–∞\n",
    "        try:\n",
    "            client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "        )\n",
    "        print(f\"üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –≤–µ–∫—Ç–æ—Ä–∞: {vector_size}\")\n",
    "    else:\n",
    "        print(\"‚ùå –í –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\")\n",
    "        return 0\n",
    "    \n",
    "    points = []\n",
    "    successful_chunks = 0\n",
    "    total_loaded = 0\n",
    "    skipped_chunks = 0\n",
    "    \n",
    "    print(\"üì§ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏...\")\n",
    "    \n",
    "    for item in tqdm(data, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤\"):\n",
    "        try:\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π\n",
    "            if 'embedding' not in item or not item['embedding']:\n",
    "                skipped_chunks += 1\n",
    "                continue\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞\n",
    "            if len(item['embedding']) != vector_size:\n",
    "                print(f\"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(item['embedding'])} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {vector_size})\")\n",
    "                skipped_chunks += 1\n",
    "                continue\n",
    "            \n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –£–ù–ò–ö–ê–õ–¨–ù–´–ô ID –≤–º–µ—Å—Ç–æ chunk_id\n",
    "            unique_id = item.get('unique_id', len(points) + total_loaded)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant\n",
    "            point = {\n",
    "                \"id\": unique_id,\n",
    "                \"vector\": item[\"embedding\"],\n",
    "                \"payload\": {\n",
    "                    \"text\": item.get(\"text\", \"\"),\n",
    "                    \"title\": item.get(\"title\", \"\"),\n",
    "                    \"source\": item.get(\"source\", \"\"),\n",
    "                    \"word_count\": item.get(\"word_count\", 0),\n",
    "                    \"chunk_id\": item.get(\"chunk_id\", 0),\n",
    "                    \"file_source\": item.get(\"file_source\", \"unknown\"),\n",
    "                    \"unique_id\": unique_id\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ JSON –≤ payload\n",
    "            for key, value in item.items():\n",
    "                if key not in [\"embedding\", \"chunk_id\", \"unique_id\"] and key not in point[\"payload\"]:\n",
    "                    point[\"payload\"][key] = value\n",
    "            \n",
    "            points.append(point)\n",
    "            successful_chunks += 1\n",
    "            \n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ç—á–∞–º–∏\n",
    "            if len(points) >= batch_size:\n",
    "                print(f\"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ç—á –∏–∑ {len(points)} —Ç–æ—á–µ–∫...\")\n",
    "                try:\n",
    "                    client.upsert(\n",
    "                        collection_name=collection_name,\n",
    "                        points=points\n",
    "                    )\n",
    "                    total_loaded += len(points)\n",
    "                    print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} —á–∞–Ω–∫–æ–≤ (–≤—Å–µ–≥–æ: {total_loaded})\")\n",
    "                    points = []  # –û—á–∏—â–∞–µ–º points –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞: {e}\")\n",
    "                    # –ü—Ä–æ–±—É–µ–º —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "                    if batch_size > 10:\n",
    "                        batch_size = max(10, batch_size // 2)\n",
    "                        print(f\"üîÑ –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–æ {batch_size}\")\n",
    "                    continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ ID {item.get('unique_id', 'unknown')}: {e}\")\n",
    "            skipped_chunks += 1\n",
    "            continue\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ—á–∫–∏\n",
    "    if points:\n",
    "        print(f\"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á –∏–∑ {len(points)} —Ç–æ—á–µ–∫...\")\n",
    "        try:\n",
    "            client.upsert(\n",
    "                collection_name=collection_name,\n",
    "                points=points\n",
    "            )\n",
    "            total_loaded += len(points)\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} —á–∞–Ω–∫–æ–≤ (–≤—Å–µ–≥–æ: {total_loaded})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±–∞—Ç—á–∞: {e}\")\n",
    "    \n",
    "    print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏:\")\n",
    "    print(f\"   ‚Ä¢ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(data)}\")\n",
    "    print(f\"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {total_loaded}\")\n",
    "    print(f\"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_chunks}\")\n",
    "    \n",
    "    return total_loaded\n",
    "\n",
    "def main():\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• JSON –î–ê–ù–ù–´–• –í QDRANT (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # –ñ–¥–µ–º Qdrant\n",
    "    wait_for_qdrant()\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é\n",
    "    collection_name = setup_json_collection()\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–ø–∫–∏ data —Å –£–ù–ò–ö–ê–õ–¨–ù–´–ú–ò ID\n",
    "    all_data = load_all_json_data_with_ids(\"data\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ\")\n",
    "        return\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ ID\n",
    "    chunks_loaded = process_and_upload_json_fixed(all_data, collection_name)\n",
    "    \n",
    "    # –ò—Ç–æ–≥–∏\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"–ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìà –ò—Ç–æ–≥–∏:\")\n",
    "    print(f\"   ‚Ä¢ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ JSON —Ñ–∞–π–ª–∞—Ö: {len(all_data)}\")\n",
    "    print(f\"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ Qdrant: {chunks_loaded}\")\n",
    "    print(f\"   ‚Ä¢ –ö–æ–ª–ª–µ–∫—Ü–∏—è: '{collection_name}'\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é\n",
    "    check_collection_info(collection_name)\n",
    "    \n",
    "    # –í–ï–†–ò–§–ò–ö–ê–¶–ò–Ø –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫\n",
    "    verify_collection_count(collection_name, len(all_data))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34399ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "class ImprovedBatchSearch:\n",
    "    def __init__(self, model_path, collection_name=\"json_chunks\"):\n",
    "        try:\n",
    "            self.client = QdrantClient(host=\"localhost\", port=6333)\n",
    "            self.collection_name = collection_name\n",
    "            \n",
    "            print(f\"–ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑: {model_path}\")\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"‚ùå –û–®–ò–ë–ö–ê: –ü—É—Ç—å '{model_path}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")\n",
    "                return\n",
    "            \n",
    "            print(\"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
    "            self.model = SentenceTransformer(model_path, device='cpu')\n",
    "            print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}\")\n",
    "            self.model = None\n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        if self.model is None:\n",
    "            return None\n",
    "        return self.model.encode([text])[0].tolist()\n",
    "    \n",
    "    def is_good_result(self, title, text, url):\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ö–æ—Ä–æ—à–∏–º\"\"\"\n",
    "        # –ü–ª–æ—Ö–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏\n",
    "        bad_titles = [\n",
    "            '–ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞', '–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è', '–ö–æ–º–∏—Å—Å–∏—è –∑–∞ –∫–≤–∞–∑–∏-–∫—ç—à', \n",
    "            'dogovor_x5', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è', 'test', '–û—Å—Ç–∞–≤–∏—Ç—å –∑–∞—è–≤–∫—É',\n",
    "            '–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'\n",
    "        ]\n",
    "        \n",
    "        # –ü–ª–æ—Ö–∏–µ URL –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "        bad_url_patterns = [\n",
    "            r'.*\\.pdf$', r'.*alfa_[\\d_]+\\.pdf$', r'.*ct-[\\d_]+\\.pdf$',\n",
    "            r'.*private_[\\d_]+\\.pdf$', r'.*current_[\\d_]+\\.pdf$',\n",
    "            r'.*balance_[\\d_]+\\.pdf$', r'.*airplane_[\\d_]+\\.pdf$'\n",
    "        ]\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "        if any(bad_title in title for bad_title in bad_titles):\n",
    "            return False\n",
    "            \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL\n",
    "        if any(re.match(pattern, url) for pattern in bad_url_patterns):\n",
    "            return False\n",
    "            \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç\n",
    "        if len(text.strip()) < 30:\n",
    "            return False\n",
    "            \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑\n",
    "        words = text.split()\n",
    "        if len(set(words)) < 5:  # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 5 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def search_high_quality_results(self, query, limit=5):\n",
    "        \"\"\"–ü–æ–∏—Å–∫ –í–´–°–û–ö–û–ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\"\"\"\n",
    "        if self.model is None:\n",
    "            return []\n",
    "            \n",
    "        query_embedding = self.get_embedding(query)\n",
    "        \n",
    "        if query_embedding is None:\n",
    "            return []\n",
    "        \n",
    "        # –ò—â–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –≤—ã–±–æ—Ä–∞\n",
    "        results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit * 20  # –ò—â–µ–º –≤ 10 —Ä–∞–∑ –±–æ–ª—å—à–µ\n",
    "        )\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        high_quality_results = []\n",
    "        seen_urls = set()\n",
    "        seen_titles = set()\n",
    "        \n",
    "        for result in results.points:\n",
    "            if len(high_quality_results) >= limit:\n",
    "                break\n",
    "                \n",
    "            url = result.payload.get('source', '')\n",
    "            title = result.payload.get('title', '').strip()\n",
    "            text = result.payload.get('text', '').strip()\n",
    "            \n",
    "            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "            if url in seen_urls or title in seen_titles:\n",
    "                continue\n",
    "                \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "            if self.is_good_result(title, text, url):\n",
    "                seen_urls.add(url)\n",
    "                seen_titles.add(title)\n",
    "                \n",
    "                high_quality_results.append({\n",
    "                    'score': result.score,\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'text': text,\n",
    "                    'chunk_id': result.payload.get('chunk_id', ''),\n",
    "                    'word_count': result.payload.get('word_count', '')\n",
    "                })\n",
    "        \n",
    "        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä\n",
    "        if len(high_quality_results) < limit:\n",
    "            return self.search_with_broader_filter(query, limit, high_quality_results)\n",
    "        \n",
    "        return high_quality_results\n",
    "    \n",
    "    def search_with_broader_filter(self, query, limit, existing_results):\n",
    "        \"\"\"–ü–æ–∏—Å–∫ —Å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏\"\"\"\n",
    "        if self.model is None:\n",
    "            return existing_results\n",
    "            \n",
    "        query_embedding = self.get_embedding(query)\n",
    "        \n",
    "        if query_embedding is None:\n",
    "            return existing_results\n",
    "        \n",
    "        # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        additional_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit * 10\n",
    "        )\n",
    "        \n",
    "        seen_urls = {result['url'] for result in existing_results}\n",
    "        seen_titles = {result['title'] for result in existing_results}\n",
    "        \n",
    "        # –ú–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏\n",
    "        mild_bad_titles = ['–ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è', 'test']\n",
    "        \n",
    "        for result in additional_results.points:\n",
    "            if len(existing_results) >= limit:\n",
    "                break\n",
    "                \n",
    "            url = result.payload.get('source', '')\n",
    "            title = result.payload.get('title', '').strip()\n",
    "            text = result.payload.get('text', '').strip()\n",
    "            \n",
    "            # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏\n",
    "            if (url not in seen_urls and \n",
    "                title not in seen_titles and\n",
    "                not any(bad in title for bad in mild_bad_titles) and\n",
    "                len(text) > 20 and\n",
    "                not re.match(r'.*\\.pdf$', url)):\n",
    "                \n",
    "                seen_urls.add(url)\n",
    "                seen_titles.add(title)\n",
    "                \n",
    "                existing_results.append({\n",
    "                    'score': result.score,\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'text': text,\n",
    "                    'chunk_id': result.payload.get('chunk_id', ''),\n",
    "                    'word_count': result.payload.get('word_count', '')\n",
    "                })\n",
    "        \n",
    "        return existing_results\n",
    "\n",
    "def create_improved_csv_links_only(results, output_csv=\"search_results_improved_links.csv\"):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç CSV —Ç–æ–ª—å–∫–æ —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ 5 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏\"\"\"\n",
    "    \n",
    "    flat_data = []\n",
    "    \n",
    "    for item in results:\n",
    "        q_id = item['q_id']\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∑–∞–ø–∏—Å—å\n",
    "        record = {\n",
    "            'query_id': q_id,\n",
    "            'query': item['query']\n",
    "        }\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n",
    "        if item.get('results'):\n",
    "            for i, result in enumerate(item['results']):\n",
    "                if i < 5:  # –ú–∞–∫—Å–∏–º—É–º 5 —Å—Å—ã–ª–æ–∫\n",
    "                    record[f'link_{i+1}'] = result['url']\n",
    "                    record[f'title_{i+1}'] = result['title']\n",
    "                    record[f'score_{i+1}'] = result['score']\n",
    "        \n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—É—Å—Ç—ã–µ –ø–æ–ª—è\n",
    "        results_count = len(item.get('results', []))\n",
    "        for i in range(results_count, 5):\n",
    "            record[f'link_{i+1}'] = ''\n",
    "            record[f'title_{i+1}'] = ''\n",
    "            record[f'score_{i+1}'] = 0.0\n",
    "        \n",
    "        flat_data.append(record)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame –∏ –°–û–†–¢–ò–†–£–ï–ú –ø–æ query_id\n",
    "    df_links = pd.DataFrame(flat_data)\n",
    "    df_links = df_links.sort_values('query_id').reset_index(drop=True)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV\n",
    "    df_links.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π CSV —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_csv}\")\n",
    "    print(f\"üìä –†–∞–∑–º–µ—Ä: {len(df_links)} —Å—Ç—Ä–æ–∫ √ó {len(df_links.columns)} –∫–æ–ª–æ–Ω–æ–∫\")\n",
    "    print(f\"üî¢ –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ query_id: {df_links['query_id'].min()} - {df_links['query_id'].max()}\")\n",
    "    \n",
    "    return df_links\n",
    "\n",
    "def create_improved_excel_with_texts(results, output_excel=\"search_results_improved_texts.xlsx\"):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç Excel —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\"\"\"\n",
    "    \n",
    "    excel_data = []\n",
    "    \n",
    "    for item in results:\n",
    "        q_id = item['q_id']\n",
    "        query = item['query']\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∑–∞–ø–∏—Å—å\n",
    "        record = {\n",
    "            'query_id': q_id,\n",
    "            'query': query,\n",
    "            'results_found': len(item.get('results', []))\n",
    "        }\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Å—ã–ª–∫–µ\n",
    "        if item.get('results'):\n",
    "            for i, result in enumerate(item['results']):\n",
    "                if i < 5:  # –ú–∞–∫—Å–∏–º—É–º 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "                    record[f'link_{i+1}'] = result['url']\n",
    "                    record[f'title_{i+1}'] = result['title']\n",
    "                    record[f'score_{i+1}'] = result['score']\n",
    "                    record[f'text_{i+1}'] = result['text']\n",
    "                    record[f'word_count_{i+1}'] = result.get('word_count', '')\n",
    "        \n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—É—Å—Ç—ã–µ –ø–æ–ª—è\n",
    "        results_count = len(item.get('results', []))\n",
    "        for i in range(results_count, 5):\n",
    "            record[f'link_{i+1}'] = ''\n",
    "            record[f'title_{i+1}'] = ''\n",
    "            record[f'score_{i+1}'] = 0.0\n",
    "            record[f'text_{i+1}'] = ''\n",
    "            record[f'word_count_{i+1}'] = ''\n",
    "        \n",
    "        excel_data.append(record)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame –∏ –°–û–†–¢–ò–†–£–ï–ú –ø–æ query_id\n",
    "    df_excel = pd.DataFrame(excel_data)\n",
    "    df_excel = df_excel.sort_values('query_id').reset_index(drop=True)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel\n",
    "    with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "        df_excel.to_excel(writer, sheet_name='Search Results', index=False)\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —à–∏—Ä–∏–Ω—É –∫–æ–ª–æ–Ω–æ–∫\n",
    "        worksheet = writer.sheets['Search Results']\n",
    "        worksheet.column_dimensions['A'].width = 10  # query_id\n",
    "        worksheet.column_dimensions['B'].width = 50  # query\n",
    "        worksheet.column_dimensions['C'].width = 12  # results_found\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —à–∏—Ä–∏–Ω—É –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "        for col in range(4, len(df_excel.columns) + 1):\n",
    "            col_letter = chr(64 + col) if col <= 26 else 'A' + chr(64 + col - 26)\n",
    "            if 'text' in df_excel.columns[col-1]:\n",
    "                worksheet.column_dimensions[col_letter].width = 60\n",
    "            elif 'link' in df_excel.columns[col-1]:\n",
    "                worksheet.column_dimensions[col_letter].width = 35\n",
    "            elif 'title' in df_excel.columns[col-1]:\n",
    "                worksheet.column_dimensions[col_letter].width = 25\n",
    "            else:\n",
    "                worksheet.column_dimensions[col_letter].width = 12\n",
    "    \n",
    "    print(f\"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π Excel —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_excel}\")\n",
    "    print(f\"üìä –†–∞–∑–º–µ—Ä: {len(df_excel)} —Å—Ç—Ä–æ–∫ √ó {len(df_excel.columns)} –∫–æ–ª–æ–Ω–æ–∫\")\n",
    "    print(f\"üî¢ –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ query_id: {df_excel['query_id'].min()} - {df_excel['query_id'].max()}\")\n",
    "    \n",
    "    return df_excel\n",
    "\n",
    "def process_dataframe_improved(df, model_path, sample_size=None):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç DataFrame —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º\"\"\"\n",
    "    \n",
    "    searcher = ImprovedBatchSearch(model_path)\n",
    "    if searcher.model is None:\n",
    "        return None\n",
    "    \n",
    "    if sample_size and sample_size < len(df):\n",
    "        df_sample = df.sample(sample_size, random_state=42)\n",
    "        print(f\"üß™ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ {sample_size} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "    else:\n",
    "        df_sample = df\n",
    "        print(f\"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "    \n",
    "    # –°–û–†–¢–ò–†–£–ï–ú –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame –ø–æ q_id –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π\n",
    "    df_sample = df_sample.sort_values('q_id').reset_index(drop=True)\n",
    "    print(f\"üî¢ –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ q_id: {df_sample['q_id'].min()} - {df_sample['q_id'].max()}\")\n",
    "    \n",
    "    results = []\n",
    "    quality_stats = {\n",
    "        'total_queries': 0,\n",
    "        'queries_with_5_results': 0,\n",
    "        'queries_with_3_plus_results': 0,\n",
    "        'queries_with_0_results': 0,\n",
    "        'total_quality_results': 0\n",
    "    }\n",
    "    \n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫\"):\n",
    "        query = row['query']\n",
    "        q_id = row['q_id']\n",
    "        \n",
    "        try:\n",
    "            search_results = searcher.search_high_quality_results(query, limit=5)\n",
    "            \n",
    "            result_item = {\n",
    "                'q_id': q_id,\n",
    "                'query': query,\n",
    "                'results': search_results,\n",
    "                'results_count': len(search_results)\n",
    "            }\n",
    "            \n",
    "            results.append(result_item)\n",
    "            \n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "            results_count = len(search_results)\n",
    "            quality_stats['total_quality_results'] += results_count\n",
    "            \n",
    "            if results_count == 5:\n",
    "                quality_stats['queries_with_5_results'] += 1\n",
    "            elif results_count >= 3:\n",
    "                quality_stats['queries_with_3_plus_results'] += 1\n",
    "            elif results_count == 0:\n",
    "                quality_stats['queries_with_0_results'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {q_id}: {e}\")\n",
    "            results.append({\n",
    "                'q_id': q_id,\n",
    "                'query': query,\n",
    "                'results': [],\n",
    "                'results_count': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            quality_stats['queries_with_0_results'] += 1\n",
    "    \n",
    "    quality_stats['total_queries'] = len(results)\n",
    "    \n",
    "    # –°–û–†–¢–ò–†–£–ï–ú —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ q_id –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º\n",
    "    results = sorted(results, key=lambda x: x['q_id'])\n",
    "    print(f\"üî¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ q_id: {results[0]['q_id']} - {results[-1]['q_id']}\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –æ–±–∞ —Ñ–∞–π–ª–∞\n",
    "    print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...\")\n",
    "    \n",
    "    # 1. CSV —Ç–æ–ª—å–∫–æ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏\n",
    "    df_csv = create_improved_csv_links_only(results)\n",
    "    \n",
    "    # 2. Excel —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π\n",
    "    df_excel = create_improved_excel_with_texts(results)\n",
    "    \n",
    "    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    print(f\"\\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ê–ß–ï–°–¢–í–ê:\")\n",
    "    print(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {quality_stats['total_queries']}\")\n",
    "    print(f\"   –ó–∞–ø—Ä–æ—Å–æ–≤ —Å 5 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {quality_stats['queries_with_5_results']}\")\n",
    "    print(f\"   –ó–∞–ø—Ä–æ—Å–æ–≤ —Å 3+ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {quality_stats['queries_with_3_plus_results']}\")\n",
    "    print(f\"   –ó–∞–ø—Ä–æ—Å–æ–≤ —Å 0 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {quality_stats['queries_with_0_results']}\")\n",
    "    print(f\"   –í—Å–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {quality_stats['total_quality_results']}\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {quality_stats['total_quality_results']/quality_stats['total_queries']:.2f}\")\n",
    "    \n",
    "    return df_csv, df_excel, results, quality_stats\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"–£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö - –°–û–†–¢–ò–†–û–í–ö–ê –ü–û ID\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    try:\n",
    "        df = pd.read_csv('questions_clean.csv')\n",
    "        print(f\"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω CSV: {len(df)} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "        print(f\"üî¢ –î–∏–∞–ø–∞–∑–æ–Ω ID –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ: {df['q_id'].min()} - {df['q_id'].max()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}\")\n",
    "        return\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º\n",
    "    model_path = \"local_sbert_with_triplets\"\n",
    "    df_csv, df_excel, results, stats = process_dataframe_improved(df, model_path, sample_size=10_000)\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    if df_csv is not None and df_excel is not None:\n",
    "        print(f\"\\nüéØ –ü–†–ò–ú–ï–†–´ –£–õ–£–ß–®–ï–ù–ù–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–ø–µ—Ä–≤—ã–µ 5 –ø–æ –ø–æ—Ä—è–¥–∫—É):\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É ID\n",
    "        for i, item in enumerate(results[:5]):\n",
    "            print(f\"\\n--- –ó–∞–ø—Ä–æ—Å {item['q_id']} (–ø–æ—Ä—è–¥–∫–æ–≤—ã–π ‚Ññ{i+1}) ---\")\n",
    "            print(f\"üìã –ó–∞–ø—Ä–æ—Å: '{item['query']}'\")\n",
    "            print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {item['results_count']}\")\n",
    "    \n",
    "            for j, result in enumerate(item['results']):\n",
    "                print(f\"  {j+1}. [{result['score']:.4f}] {result['title']}\")\n",
    "                print(f\"     üîó {result['url']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6bb3c",
   "metadata": {},
   "source": [
    "# –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b77583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('input_data/search_results_improved_links_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web = pd.read_csv('input_data/websites.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞ URL -> web_id\n",
    "print(\"üîÑ –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –º–∞–ø–ø–∏–Ω–≥–∞ URL -> web_id...\")\n",
    "url_to_web_id = dict(zip(df_web['url'], df_web['web_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa870b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞ –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏\n",
    "def map_url_to_web_id(url):\n",
    "    \"\"\"–ú–∞–ø–ø–∏—Ç URL –Ω–∞ web_id, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω\"\"\"\n",
    "    if pd.isna(url) or url == '':\n",
    "        return None\n",
    "    return url_to_web_id.get(url.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏\n",
    "def process_row(row):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ web_id\"\"\"\n",
    "    web_ids = []\n",
    "    \n",
    "    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º link –∫–æ–ª–æ–Ω–∫–∞–º\n",
    "    for i in range(1, 6):\n",
    "        link_col = f'link_{i}'\n",
    "        url = row[link_col]\n",
    "        web_id = map_url_to_web_id(url)\n",
    "        \n",
    "        if web_id is not None:\n",
    "            web_ids.append(web_id)\n",
    "    \n",
    "    return web_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19727945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É\n",
    "print(\"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ URL -> web_id...\")\n",
    "df_final['web_list'] = df_final.apply(process_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e30b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "result_df = pd.DataFrame({\n",
    "    'q_id': df_final['query_id'],\n",
    "    'web_list': df_final['web_list'].apply(lambda x: str(x).replace(\"'\", \"\"))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f99f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "result_df_with_random_id = result_df.copy()\n",
    "\n",
    "np.random.seed(42)\n",
    "result_df_with_random_id['web_list'] = result_df_with_random_id['web_list'].apply(\n",
    "    lambda x: str(\n",
    "        eval(x) + [np.random.randint(1, 1501) for _ in range(5 - len(eval(x)))]\n",
    "    ).replace(\"'\", \"\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ –ì–æ—Ç–æ–≤–æ –∑–∞ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_with_random_id = result_df_with_random_id.rename(columns={'web_list': 'web_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def safe_convert_to_lists(result_df_with_random_id):\n",
    "    \"\"\"\n",
    "    –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å ast.literal_eval\n",
    "    \"\"\"\n",
    "    def safe_convert(value):\n",
    "        if isinstance(value, list):\n",
    "            return value\n",
    "        try:\n",
    "            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤ Python –æ–±—ä–µ–∫—Ç\n",
    "            return ast.literal_eval(value)\n",
    "        except:\n",
    "            try:\n",
    "                # –ü—Ä–æ–±—É–µ–º —É–±—Ä–∞—Ç—å –∫–∞–≤—ã—á–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å\n",
    "                clean_value = value.strip('\"')\n",
    "                return ast.literal_eval(clean_value)\n",
    "            except:\n",
    "                print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å: {value}\")\n",
    "                return [1, 2, 3, 4, 5]\n",
    "    \n",
    "    result_df_with_random_id['web_id'] = result_df_with_random_id['web_id'].apply(safe_convert)\n",
    "    return result_df_with_random_id\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "result_df_with_random_id = safe_convert_to_lists(result_df_with_random_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_with_random_id = result_df_with_random_id.rename(columns={'web_id': 'web_list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_with_random_id.to_csv('output_data/submit_new_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
